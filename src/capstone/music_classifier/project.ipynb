{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project file for the music classifier project for the MIT AI Capstone project.\n",
    " \n",
    "The model is a CNN that takes in a mel-spectrogram image and outputs the mood of the song. For now the genre is not used in the model.\n",
    "It will be trained on a dataset created from my personal music library and the mel-spectrogram images will be generated using the \n",
    "librosa library.\n",
    "\n",
    "The dataset generation is done using a shell script and a python program. The shell script goes through specific music directories\n",
    "and generates a CSV file containing the filename, genre, and mood of the song. The python program will then read the CSV file and generate\n",
    "the mel-spectrogram images in a directory and create another CSV file containing the image filename, genre, and mood, which is then used\n",
    "by the CNN model to train and test the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time, copy\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn.metrics as metrics\n",
    "import torchvision.datasets\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "from torch.utils.data import Subset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.modules.flatten import Flatten\n",
    "from torchvision.transforms import v2\n",
    "\n",
    "# device config (train our model on GPU if it is available which is much faster)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "# Mood and Genre (not used at present for classification) are categorical variables. We will use the following encoding:\n",
    "#\n",
    "# Mood:\n",
    "# 0 - Mellow\n",
    "# 1 - Not Mellow\n",
    "# 2 - Chill (not used)\n",
    "# 3 - Danceable (not used)\n",
    "#\n",
    "# Genre: (NOT USED)\n",
    "# 0 - Jazz\n",
    "# 1 - Jazz with Vocals\n",
    "# 2 - Classical\n",
    "# 3 - Pop\n",
    "# 4 - Rock\n",
    "# 5 - World\n",
    "# 6 - Electronica\n",
    "# 7 - Other\n",
    "###\n",
    "\n",
    "class SongImageDataset(Dataset):\n",
    "    def __init__(self, csv_file, root_dir, transform=None):\n",
    "        self.song_frame = pd.read_csv(csv_file, dtype={'image':str, 'genre':np.int64, 'mood':np.int64})\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.song_frame)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        if torch.is_tensor(index):\n",
    "            index = index.tolist()\n",
    "\n",
    "        img_file = self.song_frame.iloc[index, 0]\n",
    "        img_path = os.path.join(self.root_dir, img_file)\n",
    "\n",
    "        # if os.path.isfile(img_path):\n",
    "        #     print(img_path)\n",
    "        # else:\n",
    "        #     print(\"File not found\")\n",
    "        #     exit(1)\n",
    "\n",
    "        image = plt.imread(img_path)\n",
    "        genre = self.song_frame.iloc[index, 1]\n",
    "        mood = self.song_frame.iloc[index, 2]\n",
    "        \n",
    "        return self.transform(image), mood, genre"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add transforms and instantiate the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "410\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/site-packages/torchvision/transforms/v2/_deprecated.py:43: UserWarning: The transform `ToTensor()` is deprecated and will be removed in a future release. Instead, please use `v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)])`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "transform = v2.Compose([\n",
    "    # v2.Resize(size=(300, 1200), antialias=True),\n",
    "    # v2.ToDtype(torch.float32, scale=True),\n",
    "    v2.ToTensor(), \n",
    "    transforms.Normalize([0.5], [0.5])])\n",
    "\n",
    "csv_file='/Users/sonatin/src/AI/MIT/Capstone/music-playlist/melspec/music_melspec_data.csv'\n",
    "root_dir='/Users/sonatin/src/AI/MIT/Capstone/music-playlist/melspec'\n",
    "img_dataset = SongImageDataset(csv_file=csv_file, root_dir=root_dir, transform=transform)\n",
    "print(len(img_dataset))\n",
    "\n",
    "### debug in case you want to see the image size without a transform\n",
    "# set the param to transform=None above and uncomment the lines below\n",
    "###\n",
    "# print(img_dataset[0]['image'].shape)\n",
    "# print(img_dataset[0]['filename'])\n",
    "# print(img_dataset[0]['genre'])\n",
    "# print(img_dataset[0]['label'])\n",
    "# print(img_dataset[0]['image'].dtype)\n",
    "# plt.imshow(img_dataset[0]['image'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train:  328\n",
      "validation:  41\n",
      "test:  41\n"
     ]
    }
   ],
   "source": [
    "# dataset split\n",
    "img_train, img_val = torch.utils.data.random_split(img_dataset, [int(len(img_dataset)*0.8), int(len(img_dataset)*0.2)])\n",
    "print('train: ', len(img_train))\n",
    "\n",
    "img_val, img_test = torch.utils.data.random_split(img_val, [int(len(img_val)*0.5), int(len(img_val)*0.5)])\n",
    "print('validation: ', len(img_val))\n",
    "print('test: ', len(img_test))\n",
    "\n",
    "# img_dataset = torchvision.datasets.FashionMNIST('', train=True, transform =transform, download=True)\n",
    "\n",
    "# # We will split out train dataset into train and validation!\n",
    "# img_train, img_val = torch.utils.data.random_split(img_dataset, [int(np.floor(len(img_dataset)*0.75)), int(np.ceil(len(img_dataset)*0.25))])\n",
    "\n",
    "# img_test = torchvision.datasets.FashionMNIST('', train=False, transform = transform, download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset_sizes = {'train': 328, 'val': 41, 'test': 41}\n"
     ]
    }
   ],
   "source": [
    "# dataloaders\n",
    "batch_size = 32\n",
    "dataloaders = {'train': DataLoader(img_train, batch_size=batch_size),\n",
    "               'val': DataLoader(img_val, batch_size=batch_size),\n",
    "               'test': DataLoader(img_test, batch_size=batch_size)}\n",
    "\n",
    "dataset_sizes = {'train': len(img_train),\n",
    "                 'val': len(img_val),\n",
    "                 'test': len(img_test)}\n",
    "print(f'dataset_sizes = {dataset_sizes}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNClassifier(nn.Module):\n",
    "    def __init__(self, dropout = 0.25):\n",
    "        super(CNNClassifier, self).__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.conv0 = nn.Conv2d(in_channels = 4, out_channels = 8, kernel_size = 3, stride = 1, padding = 2)\n",
    "        self.conv1 = nn.Conv2d(in_channels = 8, out_channels = 8, kernel_size = 3, stride = 1, padding = 2)\n",
    "        self.conv2 = nn.Conv2d(in_channels = 8, out_channels = 16, kernel_size = 3, stride = 1, padding = 2)\n",
    "        self.conv3 = nn.Conv2d(in_channels = 16, out_channels = 16, kernel_size = 3, stride = 1, padding = 2)\n",
    "        self.conv4 = nn.Conv2d(in_channels = 16, out_channels = 32, kernel_size = 3, stride = 1, padding = 2)\n",
    "        self.conv5 = nn.Conv2d(in_channels = 32, out_channels = 32, kernel_size = 3, stride = 1, padding = 2)\n",
    "\n",
    "        self.pool = nn.MaxPool2d(kernel_size = 2, stride = 2, padding = 0)\n",
    "\n",
    "        self.batchnorm0 = nn.BatchNorm2d(8)\n",
    "        self.batchnorm1 = nn.BatchNorm2d(16)\n",
    "        self.batchnorm2 = nn.BatchNorm2d(32)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "        self.fc0 = nn.Linear(6912, 32)\n",
    "        self.fc1 = nn.Linear(32, 8)\n",
    "        self.fc2 = nn.Linear(8, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv0(x)\n",
    "        x = self.batchnorm0(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        x = self.conv1(x)\n",
    "        x = self.batchnorm0(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.pool(x)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = self.batchnorm1(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        x = self.conv3(x)\n",
    "        x = self.batchnorm1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.pool(x)\n",
    "\n",
    "        x = self.conv4(x)\n",
    "        x = self.batchnorm2(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        x = self.conv5(x)\n",
    "        x = self.batchnorm2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.pool(x)\n",
    "\n",
    "        x = self.conv5(x)\n",
    "        x = self.batchnorm2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.pool(x)\n",
    "\n",
    "        x = self.conv5(x)\n",
    "        x = self.batchnorm2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.pool(x)\n",
    "\n",
    "        x = self.conv5(x)\n",
    "        x = self.batchnorm2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.pool(x)\n",
    "\n",
    "        x = self.flatten(x) # x.view(-1, 32 * 100 * 25)  # Adjust the size based on your image dimensions\n",
    "        x = self.relu(self.fc0(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training function\n",
    "# From https://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html\n",
    "def train_classification_model(model, dataloaders, dataset_sizes, criterion, optimizer, scheduler, num_epochs=25):\n",
    "    since = time.time()\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict()) # keep the best weights stored separately\n",
    "    best_acc = 0.0\n",
    "    best_epoch = 0\n",
    "\n",
    "    # Each epoch has a training, validation, and test phase\n",
    "    phases = ['train', 'val', 'test']\n",
    "\n",
    "    # Keep track of how loss and accuracy evolves during training\n",
    "    training_curves = {}\n",
    "    for phase in phases:\n",
    "        training_curves[phase+'_loss'] = []\n",
    "        training_curves[phase+'_acc'] = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'\\nEpoch {epoch+1}/{num_epochs}')\n",
    "        print('-' * 10)\n",
    "\n",
    "        if epoch % 1 == 0:\n",
    "            print('Randomize the dataset')\n",
    "            img_train, img_val = torch.utils.data.random_split(img_dataset, [int(len(img_dataset)*0.8), int(len(img_dataset)*0.2)])\n",
    "            # print('train: ', len(img_train))\n",
    "\n",
    "            img_val, img_test = torch.utils.data.random_split(img_val, [int(len(img_val)*0.5), int(len(img_val)*0.5)])\n",
    "            # print('validation: ', len(img_val))\n",
    "            # print('test: ', len(img_test))\n",
    "\n",
    "            # We will create DataLoaders just like before with a batch size of 100\n",
    "            dataloaders = {'train': DataLoader(img_train, batch_size=batch_size),\n",
    "                        'val': DataLoader(img_val, batch_size=batch_size),\n",
    "                        'test': DataLoader(img_test, batch_size=batch_size)}\n",
    "\n",
    "            dataset_sizes = {'train': len(img_train),\n",
    "                            'val': len(img_val),\n",
    "                            'test': len(img_test)}\n",
    "\n",
    "        for phase in phases:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            for inputs, labels, other_labels in dataloaders[phase]:\n",
    "                # No need to flatten the inputs!\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    _, predictions = torch.max(outputs, 1)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    # backward + update weights only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(predictions == labels.data)\n",
    "\n",
    "            if phase == 'train':\n",
    "                scheduler.step()\n",
    "\n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
    "            training_curves[phase+'_loss'].append(epoch_loss)\n",
    "            training_curves[phase+'_acc'].append(epoch_acc)\n",
    "\n",
    "            print(f'{phase:5} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
    "\n",
    "            # deep copy the model if it's the best accuracy (bas\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "              best_epoch = epoch\n",
    "              best_acc = epoch_acc\n",
    "              best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print(f'\\nTraining complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n",
    "    print(f'Best val Acc: {best_acc:4f} at epoch {best_epoch}')\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "\n",
    "    return model, training_curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classification function\n",
    "def classify_predictions(model, device, dataloader):\n",
    "    model.eval()   # Set model to evaluate mode\n",
    "    all_labels = torch.tensor([]).to(device)\n",
    "    all_scores = torch.tensor([]).to(device)\n",
    "    all_preds = torch.tensor([]).to(device)\n",
    "    for inputs, labels, other_labels in dataloader:\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = torch.softmax(model(inputs), dim=1)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        scores = outputs[:,1]\n",
    "        all_labels = torch.cat((all_labels, labels), 0)\n",
    "        all_scores = torch.cat((all_scores, scores), 0)\n",
    "        all_preds = torch.cat((all_preds, preds), 0)\n",
    "    return all_preds.detach().cpu(), all_labels.detach().cpu(), all_scores.detach().cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_curves(training_curves,\n",
    "                         phases=['train', 'val', 'test'],\n",
    "                         metrics=['loss','acc']):\n",
    "    epochs = list(range(len(training_curves['train_loss'])))\n",
    "    for metric in metrics:\n",
    "        plt.figure()\n",
    "        plt.title(f'Training curves - {metric}')\n",
    "        for phase in phases:\n",
    "            key = phase+'_'+metric\n",
    "            if key in training_curves:\n",
    "                if metric == 'acc':\n",
    "                    plt.plot(epochs, [item.detach().cpu() for item in training_curves[key]])\n",
    "                else:\n",
    "                    plt.plot(epochs, training_curves[key])\n",
    "        plt.xlabel('epoch')\n",
    "        plt.legend(labels=phases)\n",
    "\n",
    "def plot_cm(model, device, dataloaders, phase='test'):\n",
    "    preds, labels, scores = classify_predictions(model, device, dataloaders[phase])\n",
    "\n",
    "    class_labels = labels.unique().tolist()\n",
    "    print('class_labels: ', class_labels)\n",
    "    print('preds: ', preds)\n",
    "    print('labels: ', labels)\n",
    "    cm = metrics.confusion_matrix(labels, preds)\n",
    "    disp = metrics.ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_labels)\n",
    "    ax = disp.plot().ax_\n",
    "    ax.set_title('Confusion Matrix -- counts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # plot a digit ground truth and autoencoding\n",
    "# def view_melspec(genre, mood, count = 1):\n",
    "#     fig, ax = plt.subplots(nrows=count, sharex=True, figsize=(20, 5 * count))\n",
    "#     idx = 0\n",
    "#     for inputs, genres, moods in dataloaders[\"test\"]:\n",
    "#         for i, input in enumerate(inputs):\n",
    "#             # we only want to view a certain class\n",
    "#             if (moods[i] != mood and genres[i] != genre):\n",
    "#                 continue\n",
    "#             # plot the ground truth\n",
    "#             ax[0].imshow(input, cmap='magma')\n",
    "#             idx += 1\n",
    "#             if idx >= count:\n",
    "#                 break\n",
    "#         if idx >= count:\n",
    "#             break\n",
    "\n",
    "# view_melspec(0, 0, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNNClassifier(\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (conv0): Conv2d(4, 8, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2))\n",
      "  (conv1): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2))\n",
      "  (conv2): Conv2d(8, 16, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2))\n",
      "  (conv3): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2))\n",
      "  (conv4): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2))\n",
      "  (conv5): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2))\n",
      "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (batchnorm0): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (batchnorm1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (batchnorm2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU()\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (fc0): Linear(in_features=22032, out_features=32, bias=True)\n",
      "  (fc1): Linear(in_features=32, out_features=8, bias=True)\n",
      "  (fc2): Linear(in_features=8, out_features=2, bias=True)\n",
      ")\n",
      "\n",
      "Epoch 1/10\n",
      "----------\n",
      "Randomize the dataset\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (32x6912 and 22032x32)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[70], line 13\u001b[0m\n\u001b[1;32m     10\u001b[0m scheduler \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mlr_scheduler\u001b[38;5;241m.\u001b[39mExponentialLR(optimizer, gamma\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.95\u001b[39m)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Train the model. We also will store the results of training to visualize\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m model, training_curves \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_classification_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset_sizes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_epochs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m plot_training_curves(training_curves, phases\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     16\u001b[0m res \u001b[38;5;241m=\u001b[39m plot_cm(model, device, dataloaders, phase\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[62], line 61\u001b[0m, in \u001b[0;36mtrain_classification_model\u001b[0;34m(model, dataloaders, dataset_sizes, criterion, optimizer, scheduler, num_epochs)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;66;03m# forward\u001b[39;00m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(phase \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m---> 61\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     62\u001b[0m     _, predictions \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmax(outputs, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     63\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[69], line 70\u001b[0m, in \u001b[0;36mCNNClassifier.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     67\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpool(x)\n\u001b[1;32m     69\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mflatten(x) \u001b[38;5;66;03m# x.view(-1, 32 * 100 * 25)  # Adjust the size based on your image dimensions\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfc0\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     71\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(x)\n\u001b[1;32m     72\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc1(x))\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (32x6912 and 22032x32)"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.001\n",
    "num_epochs = 10\n",
    "\n",
    "model = CNNClassifier(dropout = 0.1).to(device)\n",
    "print(model)\n",
    "\n",
    "# loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss() # CrossEntropyLoss for classification!\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=0.0001)\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.95)\n",
    "\n",
    "# Train the model. We also will store the results of training to visualize\n",
    "model, training_curves = train_classification_model(model, dataloaders, dataset_sizes, criterion, optimizer, scheduler, num_epochs=num_epochs)\n",
    "\n",
    "plot_training_curves(training_curves, phases=['train', 'val', 'test'])\n",
    "res = plot_cm(model, device, dataloaders, phase='test')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
