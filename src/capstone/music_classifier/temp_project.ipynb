{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project file for the music classifier project for the MIT AI Capstone project.\n",
    " \n",
    "The model is a CNN that takes in a mel-spectrogram image and outputs the mood of the song. For now the genre is not used in the model.\n",
    "It will be trained on a dataset created from my personal music library and the mel-spectrogram images will be generated using the \n",
    "librosa library.\n",
    "\n",
    "The dataset generation is done using a shell script and a python program. The shell script goes through specific music directories\n",
    "and generates a CSV file containing the filename, genre, and mood of the song. The python program will then read the CSV file and generate\n",
    "the mel-spectrogram images in a directory and create another CSV file containing the image filename, genre, and mood, which is then used\n",
    "by the CNN model to train and test the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time, copy\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn.metrics as metrics\n",
    "import torchvision.datasets\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "from torch.utils.data import Subset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.modules.flatten import Flatten\n",
    "from torchvision.transforms import v2\n",
    "\n",
    "# device config (train our model on GPU if it is available which is much faster)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "# Mood and Genre (not used at present for classification) are categorical variables. We will use the following encoding:\n",
    "#\n",
    "# Mood:\n",
    "# 0 - Mellow\n",
    "# 1 - Not Mellow\n",
    "# 2 - Chill (not used)\n",
    "# 3 - Danceable (not used)\n",
    "#\n",
    "# Genre: (NOT USED)\n",
    "# 0 - Jazz\n",
    "# 1 - Jazz with Vocals\n",
    "# 2 - Classical\n",
    "# 3 - Pop\n",
    "# 4 - Rock\n",
    "# 5 - World\n",
    "# 6 - Electronica\n",
    "# 7 - Other\n",
    "###\n",
    "\n",
    "class SongImageDataset(Dataset):\n",
    "    def __init__(self, csv_file, root_dir, transform=None):\n",
    "        self.song_frame = pd.read_csv(csv_file, dtype={'image':str, 'genre':np.int64, 'mood':np.int64})\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.song_frame)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        if torch.is_tensor(index):\n",
    "            index = index.tolist()\n",
    "\n",
    "        img_file = self.song_frame.iloc[index, 0]\n",
    "        img_path = os.path.join(self.root_dir, img_file)\n",
    "\n",
    "        # if os.path.isfile(img_path):\n",
    "        #     print(img_path)\n",
    "        # else:\n",
    "        #     print(\"File not found\")\n",
    "        #     exit(1)\n",
    "\n",
    "        image = plt.imread(img_path)\n",
    "        genre = self.song_frame.iloc[index, 1]\n",
    "        mood = self.song_frame.iloc[index, 2]\n",
    "        \n",
    "        return self.transform(image), mood, genre"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add transforms and instantiate the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = v2.Compose([\n",
    "    v2.ToTensor(), \n",
    "    transforms.Normalize([0.5], [0.5])])\n",
    "\n",
    "# to reduce the test time, you can try to use a subset of the dataset by modifying the CSV file (or using an alternative CSV file with reduced rows)\n",
    "csv_file='/Users/sonatin/src/AI/MIT/Capstone/music-playlist/melspec/music_melspec_data.csv'\n",
    "root_dir='/Users/sonatin/src/AI/MIT/Capstone/music-playlist/melspec'\n",
    "img_dataset = SongImageDataset(csv_file=csv_file, root_dir=root_dir, transform=transform)\n",
    "print(len(img_dataset))\n",
    "\n",
    "# debug in case you want to see the image size without a transform\n",
    "# set the param to transform=None above and uncomment the lines below\n",
    "# print(img_dataset[0]['image'].shape)\n",
    "# print(img_dataset[0]['filename'])\n",
    "# print(img_dataset[0]['genre'])\n",
    "# print(img_dataset[0]['label'])\n",
    "# print(img_dataset[0]['image'].dtype)\n",
    "# plt.imshow(img_dataset[0]['image'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset split\n",
    "img_train, img_val = torch.utils.data.random_split(img_dataset, [int(len(img_dataset)*0.8), int(len(img_dataset)*0.2)])\n",
    "print('train: ', len(img_train))\n",
    "\n",
    "img_val, img_test = torch.utils.data.random_split(img_val, [int(len(img_val)*0.5), int(len(img_val)*0.5)])\n",
    "print('validation: ', len(img_val))\n",
    "print('test: ', len(img_test))\n",
    "\n",
    "# img_dataset = torchvision.datasets.FashionMNIST('', train=True, transform =transform, download=True)\n",
    "\n",
    "# # We will split out train dataset into train and validation!\n",
    "# img_train, img_val = torch.utils.data.random_split(img_dataset, [int(np.floor(len(img_dataset)*0.75)), int(np.ceil(len(img_dataset)*0.25))])\n",
    "\n",
    "# img_test = torchvision.datasets.FashionMNIST('', train=False, transform = transform, download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataloaders\n",
    "batch_size = 8\n",
    "dataloaders = {'train': DataLoader(img_train, batch_size=batch_size),\n",
    "               'val': DataLoader(img_val, batch_size=batch_size),\n",
    "               'test': DataLoader(img_test, batch_size=batch_size)}\n",
    "\n",
    "dataset_sizes = {'train': len(img_train),\n",
    "                 'val': len(img_val),\n",
    "                 'test': len(img_test)}\n",
    "print(f'dataset_sizes = {dataset_sizes}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MusicClassifier(nn.Module):\n",
    "    def __init__(self, num_classes = 2, dropout = 0.25):\n",
    "        super(MusicClassifier, self).__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # Convolutional layers\n",
    "        self.conv0 = nn.Conv2d(in_channels=4, out_channels=32, kernel_size=3, padding=1)\n",
    "        self.conv1 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, padding=1)\n",
    "\n",
    "        # Max-pooling layers\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        # Flattening layer\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(256 * 100 * 25, 512) # outchannels * (reduced_img_width * reduced_img_height)\n",
    "        self.fc2 = nn.Linear(512, num_classes)\n",
    "\n",
    "        # Activation function\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        ####\n",
    "        # 4 Convolution and 4 MaxPool layers with 2 fully connected layers.\n",
    "        # We need to reduce the image size from 1600x400 100x25.\n",
    "        ####\n",
    "        x = self.relu(self.conv0(x))\n",
    "        x = self.pool(x)\n",
    "\n",
    "        x = self.relu(self.conv1(x))\n",
    "        x = self.pool(x)\n",
    "\n",
    "        x = self.relu(self.conv2(x))\n",
    "        x = self.pool(x)\n",
    "\n",
    "        x = self.relu(self.conv3(x))\n",
    "        x = self.pool(x)\n",
    "\n",
    "        # flatten the tensor 'x'\n",
    "        # x = x.view(-1, 256 * 100 * 25)\n",
    "        x = self.flatten(x)\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        # Final layer with softmax activation function.\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training function\n",
    "# From https://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html\n",
    "def train_classification_model(model, dataloaders, dataset_sizes, criterion, optimizer, scheduler, num_epochs=25):\n",
    "    since = time.time()\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict()) # keep the best weights stored separately\n",
    "    best_acc = 0.0\n",
    "    best_epoch = 0\n",
    "\n",
    "    # Each epoch has a training, validation, and test phase\n",
    "    phases = ['train', 'val', 'test']\n",
    "\n",
    "    # Keep track of how loss and accuracy evolves during training\n",
    "    training_curves = {}\n",
    "    for phase in phases:\n",
    "        training_curves[phase+'_loss'] = []\n",
    "        training_curves[phase+'_acc'] = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'\\nEpoch {epoch+1}/{num_epochs}')\n",
    "        print('-' * 10)\n",
    "\n",
    "        if epoch % 1 == 0:\n",
    "            print('Randomize the dataset')\n",
    "            img_train, img_val = torch.utils.data.random_split(img_dataset, [int(len(img_dataset)*0.8), int(len(img_dataset)*0.2)])\n",
    "            # print('train: ', len(img_train))\n",
    "\n",
    "            img_val, img_test = torch.utils.data.random_split(img_val, [int(len(img_val)*0.5), int(len(img_val)*0.5)])\n",
    "            # print('validation: ', len(img_val))\n",
    "            # print('test: ', len(img_test))\n",
    "\n",
    "            # We will create DataLoaders just like before with a batch size of 100\n",
    "            dataloaders = {'train': DataLoader(img_train, batch_size=batch_size),\n",
    "                        'val': DataLoader(img_val, batch_size=batch_size),\n",
    "                        'test': DataLoader(img_test, batch_size=batch_size)}\n",
    "\n",
    "            dataset_sizes = {'train': len(img_train),\n",
    "                            'val': len(img_val),\n",
    "                            'test': len(img_test)}\n",
    "\n",
    "        for phase in phases:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            for inputs, labels, other_labels in dataloaders[phase]:\n",
    "                # No need to flatten the inputs!\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    _, predictions = torch.max(outputs, 1)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    # backward + update weights only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(predictions == labels.data)\n",
    "\n",
    "            if phase == 'train':\n",
    "                scheduler.step()\n",
    "\n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
    "            training_curves[phase+'_loss'].append(epoch_loss)\n",
    "            training_curves[phase+'_acc'].append(epoch_acc)\n",
    "\n",
    "            print(f'{phase:5} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
    "\n",
    "            # deep copy the model if it's the best accuracy (bas\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "              best_epoch = epoch\n",
    "              best_acc = epoch_acc\n",
    "              best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print(f'\\nTraining complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n",
    "    print(f'Best val Acc: {best_acc:4f} at epoch {best_epoch}')\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "\n",
    "    return model, training_curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classification function\n",
    "def classify_predictions(model, device, dataloader):\n",
    "    model.eval()   # Set model to evaluate mode\n",
    "    all_labels = torch.tensor([]).to(device)\n",
    "    all_scores = torch.tensor([]).to(device)\n",
    "    all_preds = torch.tensor([]).to(device)\n",
    "    for inputs, labels, other_labels in dataloader:\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = torch.softmax(model(inputs), dim=1)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        scores = outputs[:,1]\n",
    "        all_labels = torch.cat((all_labels, labels), 0)\n",
    "        all_scores = torch.cat((all_scores, scores), 0)\n",
    "        all_preds = torch.cat((all_preds, preds), 0)\n",
    "    return all_preds.detach().cpu(), all_labels.detach().cpu(), all_scores.detach().cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_curves(training_curves,\n",
    "                         phases=['train', 'val', 'test'],\n",
    "                         metrics=['loss','acc']):\n",
    "    epochs = list(range(len(training_curves['train_loss'])))\n",
    "    for metric in metrics:\n",
    "        plt.figure()\n",
    "        plt.title(f'Training curves - {metric}')\n",
    "        for phase in phases:\n",
    "            key = phase+'_'+metric\n",
    "            if key in training_curves:\n",
    "                if metric == 'acc':\n",
    "                    plt.plot(epochs, [item.detach().cpu() for item in training_curves[key]])\n",
    "                else:\n",
    "                    plt.plot(epochs, training_curves[key])\n",
    "        plt.xlabel('epoch')\n",
    "        plt.legend(labels=phases)\n",
    "\n",
    "def plot_cm(model, device, dataloaders, phase='test'):\n",
    "    preds, labels, scores = classify_predictions(model, device, dataloaders[phase])\n",
    "\n",
    "    class_labels = labels.unique().tolist()\n",
    "    print('class_labels: ', class_labels)\n",
    "    print('preds: ', preds)\n",
    "    print('labels: ', labels)\n",
    "    cm = metrics.confusion_matrix(labels, preds)\n",
    "    disp = metrics.ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_labels)\n",
    "    ax = disp.plot().ax_\n",
    "    ax.set_title('Confusion Matrix -- counts')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "num_epochs = 10\n",
    "\n",
    "model = MusicClassifier(dropout = 0.1).to(device)\n",
    "print(model)\n",
    "\n",
    "# loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss() # CrossEntropyLoss for classification!\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=0.0001)\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.95)\n",
    "\n",
    "# Train the model. We also will store the results of training to visualize\n",
    "model, training_curves = train_classification_model(model, dataloaders, dataset_sizes, criterion, optimizer, scheduler, num_epochs=num_epochs)\n",
    "\n",
    "plot_training_curves(training_curves, phases=['train', 'val', 'test'])\n",
    "res = plot_cm(model, device, dataloaders, phase='test')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
